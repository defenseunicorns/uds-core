# SPDX-License-Identifier: AGPL-3.0-or-later OR Commercial

variables:
  - name: CLUSTER_NAME
  - name: K8S_DISTRO
  - name: REGION
  - name: PERMISSIONS_BOUNDARY_NAME
  - name: PERMISSIONS_BOUNDARY_ARN
  - name: STATE_BUCKET_NAME
  - name: STATE_DYNAMODB_TABLE_NAME
  - name: STATE_KEY
  - name: AMI_ID
    default: ami-068ab6ac1cec494e0

tasks:
  - name: install-eksctl
    actions:
      - cmd: |
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/download/v0.190.0/eksctl_Linux_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin

  - name: create-cluster
    actions:
      - cmd: |
          cat <<EOF> cluster-config.yaml
          apiVersion: eksctl.io/v1alpha5
          kind: ClusterConfig

          metadata:
            name: ${CLUSTER_NAME}
            region: ${REGION}
            version: "1.30"
            tags:
              PermissionsBoundary: ${PERMISSIONS_BOUNDARY_NAME}

          iam:
            withOIDC: true
            serviceRolePermissionsBoundary: ${PERMISSIONS_BOUNDARY_ARN}

          addons:
            - name: aws-ebs-csi-driver
              attachPolicyARNs:
                - arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
              configurationValues: |-
                defaultStorageClass:
                  enabled: true
              permissionsBoundary: ${PERMISSIONS_BOUNDARY_ARN}
              tags:
                PermissionsBoundary: ${PERMISSIONS_BOUNDARY_NAME}

            - name: vpc-cni
              attachPolicyARNs:
                - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
              permissionsBoundary: ${PERMISSIONS_BOUNDARY_ARN}
              tags:
                PermissionsBoundary: ${PERMISSIONS_BOUNDARY_NAME}

          managedNodeGroups:
            - name: ng-1
              instanceType: m5.2xlarge
              desiredCapacity: 3
              volumeSize: 150
              tags:
                PermissionsBoundary: ${PERMISSIONS_BOUNDARY_NAME}
              iam:
                instanceRolePermissionsBoundary: ${PERMISSIONS_BOUNDARY_ARN}
                withAddonPolicies:
                  cloudWatch: true
                  ebs: true
          cloudWatch:
            clusterLogging:
              enableTypes: ["*"]
              logRetentionInDays: 1
          EOF

      - cmd: eksctl create cluster --dry-run -f cluster-config.yaml
      - cmd: sleep 5
      - cmd: eksctl create cluster -f cluster-config.yaml
      - cmd: eksctl utils write-kubeconfig -c ${CLUSTER_NAME}

  - name: rke2-get-kubeconfig
    actions:
      - cmd: chmod +x ./scripts/get-kubeconfig.sh && ./scripts/get-kubeconfig.sh
        dir: .github/test-infra/aws/rke2/

  - name: rke2-nodes-ready
    actions:
      - cmd: sleep 30
      - wait:
          cluster:
            kind: nodes
            condition: Ready
            name: kubernetes.io/os=linux
        maxTotalSeconds: 900

  - name: rke2-cluster-ready
    actions:
      - task: rke2-nodes-ready
      - cmd: |
          export KUBECONFIG=./rke2-config
          # wait for at least 3 nodes
          while true; do
            if [ $(uds zarf tools kubectl get nodes -o jsonpath='{range .items[*]}{.status.conditions[-1].type}={.status.conditions[-1].status}{"\n"}{end}' | egrep -i '^ready.*true' | wc -l) -lt 3 ]; then
              echo "Waiting for at least 3 nodes to be ready...";
              sleep 5;
            else
              break
            fi
          done

          # wait for cluster components
          while true; do
            if [ $(uds zarf tools kubectl get po -A --no-headers=true | egrep -v 'Running|Completed' | wc -l) -gt 0 ]; then
              echo "Waiting for cluster components to be ready...";
              sleep 5;
            else
              echo "Cluster is ready"
              break
            fi
          done
          uds zarf tools kubectl apply -f ./metallb.yaml
        dir: .github/test-infra/aws/rke2/
        maxTotalSeconds: 600

  - name: destroy-cluster
    actions:
      - cmd: eksctl delete cluster -f cluster-config.yaml --disable-nodegroup-eviction --wait

  - name: create-iac
    actions:
      - task: apply-tofu
      - task: tofu-outputs
      - task: create-uds-config

  - name: destroy-iac
    actions:
      - cmd: tofu destroy -auto-approve
        dir: .github/test-infra/aws/${K8S_DISTRO}

  - name: apply-tofu
    actions:
      - cmd: echo ${STATE_KEY} | sed 's/\.tfstate/-buckets.tfstate/g'
        setVariables:
          - name: BUCKETS_STATE_KEY
        dir: .github/test-infra/aws/${K8S_DISTRO}
      - cmd: |
          tofu init -force-copy \
            -backend-config="bucket=${STATE_BUCKET_NAME}" \
            -backend-config="key=${BUCKETS_STATE_KEY}" \
            -backend-config="region=${REGION}" \
            -backend-config="dynamodb_table=${STATE_DYNAMODB_TABLE_NAME}"
        dir: .github/test-infra/aws/${K8S_DISTRO}
      - cmd: tofu apply -auto-approve
        dir: .github/test-infra/aws/${K8S_DISTRO}

  - name: tofu-outputs
    actions:
      - cmd: tofu output -raw loki_s3_bucket
        setVariables:
          - name: "LOKI_S3_BUCKET"
        dir: .github/test-infra/aws/${K8S_DISTRO}
        mute: true
      - cmd: tofu output -raw aws_region
        setVariables:
          - name: LOKI_S3_AWS_REGION
        dir: .github/test-infra/aws/${K8S_DISTRO}
      - cmd: tofu output -raw loki_irsa_role_arn
        setVariables:
          - name: LOKI_S3_ROLE_ARN
        dir: .github/test-infra/aws/${K8S_DISTRO}
        mute: true
      - cmd: tofu output -raw velero_s3_bucket
        setVariables:
          - name: VELERO_S3_BUCKET
        dir: .github/test-infra/aws/${K8S_DISTRO}
        mute: true
      - cmd: tofu output -raw aws_region
        setVariables:
          - name: VELERO_S3_AWS_REGION
        dir: .github/test-infra/aws/${K8S_DISTRO}
      - cmd: tofu output -raw velero_irsa_role_arn
        setVariables:
          - name: VELERO_S3_ROLE_ARN
        dir: .github/test-infra/aws/${K8S_DISTRO}
        mute: true
      - cmd: tofu output -raw grafana_pg_host 2>/dev/null || echo '""'
        setVariables:
          - name: GRAFANA_PG_HOST
        dir: .github/test-infra/aws/${K8S_DISTRO}
        mute: true
      - cmd: tofu output -raw grafana_pg_port 2>/dev/null || echo '""'
        setVariables:
          - name: GRAFANA_PG_PORT
        dir: .github/test-infra/aws/${K8S_DISTRO}
        mute: true
      - cmd: tofu output -raw grafana_pg_database 2>/dev/null || echo '""'
        setVariables:
          - name: GRAFANA_PG_DATABASE
        dir: .github/test-infra/aws/${K8S_DISTRO}
        mute: true
      - cmd: tofu output -raw grafana_pg_user 2>/dev/null || echo '""'
        setVariables:
          - name: GRAFANA_PG_USER
        dir: .github/test-infra/aws/${K8S_DISTRO}
        mute: true
      - cmd: tofu output -raw grafana_pg_password 2>/dev/null || echo '""'
        setVariables:
          - name: GRAFANA_PG_PASSWORD
        mute: true # Muted to hide sensitive password
        dir: .github/test-infra/aws/${K8S_DISTRO}
      - cmd: tofu output -raw grafana_ha
        setVariables:
          - name: GRAFANA_HA
        dir: .github/test-infra/aws/${K8S_DISTRO}

  - name: create-uds-config
    actions:
      - task: tofu-outputs
      - cmd: |
          cat <<EOF> .github/bundles/${K8S_DISTRO}/uds-config.yaml
          options:
            architecture: amd64
          variables:
            core:
              loki_chunks_bucket: ${LOKI_S3_BUCKET}
              loki_ruler_bucket: ${LOKI_S3_BUCKET}
              loki_admin_bucket: ${LOKI_S3_BUCKET}
              loki_s3_region: ${LOKI_S3_AWS_REGION}
              loki_irsa_role_arn: ${LOKI_S3_ROLE_ARN}
              velero_use_secret: false
              velero_irsa_role_arn: ${VELERO_S3_ROLE_ARN}
              velero_bucket: ${VELERO_S3_BUCKET}
              velero_bucket_region: ${VELERO_S3_AWS_REGION}
              velero_bucket_provider_url: ""
              velero_bucket_credential_name: ""
              velero_bucket_credential_key: ""
              grafana_ha: ${GRAFANA_HA}
              grafana_pg_host: ${GRAFANA_PG_HOST}
              grafana_pg_port: ${GRAFANA_PG_PORT}
              grafana_pg_database: ${GRAFANA_PG_DATABASE}
              grafana_pg_password: ${GRAFANA_PG_PASSWORD}
              grafana_pg_user: ${GRAFANA_PG_USER}
          EOF
